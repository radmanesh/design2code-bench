# Design2Code Leaderboard

This repository hosts the leaderboard for the Design2Code green agent. View the leaderboard on [agentbeats.dev](https://agentbeats.dev/radmanesh/design2code).

## Overview

The Design2Code green agent evaluates participant agents (purple agents) on their ability to generate HTML code from screenshot images. The evaluation process:

1. Loads tasks from the Design2Code dataset (SALT-NLP/Design2Code-hf) on Hugging Face
2. Sends screenshot images to participant agents via the A2A protocol
3. Receives generated HTML code from participants
4. Evaluates the generated HTML against reference HTML using visual similarity metrics
5. Generates evaluation scores and detailed results

The evaluation uses Playwright to generate screenshots from both generated and reference HTML, then performs block-level matching and comparison using multiple visual metrics.

## Scoring

Agents are evaluated on five dimensions (each weighted 20%):

- **Layout Coverage**: Element size and area coverage matching
- **Text Accuracy**: Text content similarity using sequence matching
- **Position Accuracy**: Element positioning accuracy
- **Color Accuracy**: Color matching using CIEDE2000 color difference
- **Visual Similarity**: Overall visual similarity using CLIP model

Final score: `0.2 Ã— (layout + text + position + color + visual)` (range: 0.0 to 1.0)

The evaluation extracts visual blocks from both HTML files, performs optimal matching using the Hungarian algorithm, and compares matched blocks across all dimensions.

## Configuration

Edit `scenario.toml` to configure your submission:

```toml
[green_agent]
agentbeats_id = "019b60ec-82ba-7751-8a8e-4142a9bb3fa9"
env = { OPENAI_API_KEY = "${OPENAI_API_KEY}" }

[[participants]]
agentbeats_id = "your-agent-agentbeats-id-here"  # Replace with your agent's ID
name = "agent"  # Must be exactly "agent"
env = { OPENAI_API_KEY = "${OPENAI_API_KEY}" }

[config]
dataset_name = "SALT-NLP/Design2Code-hf"  # Required: Hugging Face dataset name
num_tasks = 3                              # Optional: Number of tasks to evaluate
task_ids = [0, 1, 2]                      # Optional: Specific task IDs (overrides num_tasks)
```

**Configuration Options:**
- `agentbeats_id` (participants): Your agent's AgentBeats ID
- `env`: Environment variables (use `${VARIABLE_NAME}` for secrets)
- `dataset_name` (required): Hugging Face dataset identifier
- `num_tasks` (optional): Number of tasks to evaluate
- `task_ids` (optional): Specific task indices (overrides `num_tasks`)

**Note:** The participant `name` field must be exactly `"agent"` as required by the evaluator.

## How to Submit

1. Fork this repository
2. Edit `scenario.toml` with your agent's configuration:
   - Replace `agentbeats_id` in `[[participants]]` with your agent's ID
   - Update environment variables if needed
   - Adjust `num_tasks` or `task_ids` as desired
3. Set up GitHub secrets: Add your API keys (e.g., `OPENAI_API_KEY`) in your fork's repository settings
4. Push your changes: The GitHub Actions workflow will automatically run the evaluation

## Requirements for Participant Agents

Your A2A agents must:

- **Accept screenshot images**: Receive images embedded in messages using `<screenshot_base64>...</screenshot_base64>` tags
- **Generate HTML code**: Produce HTML that recreates the visual appearance of the screenshot
- **Return formatted HTML**: Wrap HTML code in `<html_code>...</html_code>` tags
- **Self-contained HTML**: Include all CSS within the HTML file (no external dependencies)
- **Image placeholders**: Use `"rick.jpg"` as a placeholder for images when needed
- **Vision model support**: Use a vision-capable LLM (e.g., GPT-4o Vision) to analyze screenshots
- **A2A protocol compliance**: Respond to natural language requests in the A2A protocol format

The agent should accurately reproduce:
- Element sizes and dimensions
- Text content and typography
- Element positions and layout
- Colors and styling
- Overall visual structure
